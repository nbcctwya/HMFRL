# ä¸‹è½½ã€ä¿®å¤ã€æ‹†åˆ†æ•°æ®æ‰€éœ€è¦ç”¨åˆ°çš„å‡½æ•°
import sys
from pathlib import Path
# å°†é¡¹ç›®æ ¹ç›®å½•åŠ å…¥ sys.pathï¼ˆä»¥ä¾¿å¯¼å…¥ tools/ï¼‰
PROJECT_ROOT = Path(__file__).parent.parent
sys.path.insert(0, str(PROJECT_ROOT))
import os
import requests
from datetime import datetime
from dateutil.relativedelta import relativedelta
import pandas as pd
import glob
import zipfile
from pathlib import Path
import matplotlib.pyplot as plt
import yaml

def load_config(config_path="config.yaml"):
    """åŠ è½½é…ç½®æ–‡ä»¶"""
    config_path = Path(config_path)
    if not config_path.is_absolute():
        config_path = PROJECT_ROOT / config_path
    with open(config_path, "r", encoding="utf-8") as f:
        return yaml.safe_load(f)

def get_data_dirs(symbol, interval, config):
    """æ ¹æ®é…ç½®è·å–æ•°æ®ç›®å½•"""
    paths = config["paths"]
    return {
        "raw": PROJECT_ROOT / paths["raw_dir"] / symbol / interval,
        "processed": PROJECT_ROOT / paths["processed_dir"],
        "datasets": PROJECT_ROOT / paths["datasets_dir"] / f"{symbol}_{interval}"
    }


# =============== æ ¸å¿ƒå‡½æ•° ===============
# ä»Binanceä¸‹è½½åŸå§‹æ•°æ®ï¼Œæœ€å¤š3æ¬¡å°è¯•é‡æ–°ä¸‹è½½
def download_binance_klines(symbol, interval, config, logger, timeframe="monthly", max_retries=3):
    dirs = get_data_dirs(symbol, interval, config)
    # åˆ›å»ºä¿å­˜ç›®å½•
    save_dir = dirs["raw"]
    save_dir.mkdir(parents=True, exist_ok=True)
    # è§£æèµ·æ­¢æ—¶é—´
    start = datetime.strptime(config["data"]["start_date"], "%Y-%m")
    end = datetime.strptime(config["data"]["end_date"], "%Y-%m")
    # ç”ŸæˆæœŸæœ›çš„æ–‡ä»¶åˆ—è¡¨
    expected_files = []
    current = start
    while current <= end:
        filename = f"{symbol}-{interval}-{current.year}-{current.month:02d}.zip"
        expected_files.append(filename)
        current += relativedelta(months=1)

    failed_files = []
    successful_files = []

    for filename in expected_files:
        url = f"https://data.binance.vision/data/spot/{timeframe}/klines/{symbol}/{interval}/{filename}"
        filepath = save_dir / filename

        # æ£€æŸ¥æ˜¯å¦å·²å­˜åœ¨ä¸”æœ‰æ•ˆ
        if filepath.exists() and is_valid_zip(filepath):
            successful_files.append(filename)
            continue

        # å°è¯•ä¸‹è½½
        success = False
        for attempt in range(max_retries):
            try:
                logger.info(f"ğŸ“¥ ä¸‹è½½ {filename} (å°è¯• {attempt + 1}/{max_retries})")
                response = requests.get(url, stream=True, timeout=30)
                if response.status_code == 200:
                    with open(filepath, "wb") as f:
                        for chunk in response.iter_content(chunk_size=8192):
                            f.write(chunk)

                    if is_valid_zip(filepath):
                        successful_files.append(filename)
                        success = True
                        break
                    else:
                        logger.warning(f"âš ï¸ æ–‡ä»¶æ— æ•ˆ: {filename}")
                        if filepath.exists():
                            filepath.unlink()
                else:
                    logger.warning(f"âŒ HTTP {response.status_code}: {filename}")
            except Exception as e:
                logger.error(f"âš ï¸ ä¸‹è½½å¼‚å¸¸ {filename} (å°è¯• {attempt + 1}): {e}")
                if filepath.exists():
                    filepath.unlink()

        if not success:
            failed_files.append(filename)

    # æŠ¥å‘Šç»“æœ
    total_files = len(expected_files)
    success_count = len(successful_files)
    failed_count = len(failed_files)

    if failed_count == 0:
        logger.info(f"âœ… æ‰€æœ‰ {total_files} ä¸ªæ–‡ä»¶ä¸‹è½½æˆåŠŸ")
    else:
        logger.error(f"âŒ {failed_count}/{total_files} ä¸ªæ–‡ä»¶ä¸‹è½½å¤±è´¥: {failed_files}")

    return {
        "total": total_files,
        "success": success_count,
        "failed": failed_count,
        "failed_files": failed_files,
        "is_complete": failed_count == 0
    }

def is_valid_zip(filepath):
    """éªŒè¯ ZIP æ–‡ä»¶æ˜¯å¦å®Œæ•´"""
    try:
        with zipfile.ZipFile(filepath, 'r') as z:
            # æ£€æŸ¥ ZIP æ–‡ä»¶å®Œæ•´æ€§
            bad_file = z.testzip()
            return bad_file is None
    except (zipfile.BadZipFile, OSError):
        return False

# å¤šä¸ª.zipæ–‡ä»¶åˆå¹¶ä¸ºCSVæ–‡ä»¶
def merge_binance_klines(symbol, interval, config, logger):
    dirs = get_data_dirs(symbol, interval, config)
    raw_dir = dirs["raw"]
    processed_dir = dirs["processed"]
    processed_dir.mkdir(parents=True, exist_ok=True)

    # è·å–æ‰€æœ‰æœŸæœ›çš„æ–‡ä»¶ï¼ˆç°åœ¨åº”è¯¥éƒ½å­˜åœ¨ï¼‰
    start = datetime.strptime(config["data"]["start_date"], "%Y-%m")
    end = datetime.strptime(config["data"]["end_date"], "%Y-%m")
    zip_files = []
    current = start
    while current <= end:
        filename = f"{symbol}-{interval}-{current.year}-{current.month:02d}.zip"
        filepath = raw_dir / filename
        zip_files.append(str(filepath))
        current += relativedelta(months=1)

    # åˆå¹¶æ‰€æœ‰æ–‡ä»¶
    dfs = []
    for zip_path in zip_files:
        try:
            with zipfile.ZipFile(zip_path, 'r') as z:
                for csv_name in z.namelist():
                    if csv_name.endswith('.csv'):
                        with z.open(csv_name) as f:
                            df = pd.read_csv(f, header=None)
                            dfs.append(df)
        except Exception as e:
            logger.error(f"âŒ å¤„ç† ZIP æ–‡ä»¶å¤±è´¥ {zip_path}: {e}")
            return None  # ä¸¥æ ¼æ¨¡å¼ï¼šä»»ä½•ä¸€ä¸ªæ–‡ä»¶å¤„ç†å¤±è´¥å°±è¿”å› None

    if not dfs:
        logger.error("âŒ æœªæ‰¾åˆ°ä»»ä½• CSV æ–‡ä»¶")
        return None

    full_df = pd.concat(dfs, ignore_index=True)
    full_df.columns = [
        'open_time', 'open', 'high', 'low', 'close', 'volume',
        'close_time', 'quote_asset_volume', 'number_of_trades',
        'taker_buy_base_volume', 'taker_buy_quote_volume', 'ignore'
    ]

    output_file = processed_dir / f"{symbol}_{interval}_raw.csv"
    full_df.to_csv(output_file, index=False, header=True)
    logger.info(f"âœ… åˆå¹¶å®Œæˆ: {output_file} ({len(full_df)} è¡Œ)")
    return str(output_file)

# ä¿®å¤æ—¶é—´æˆ³å‡½æ•° (âœ…BTCUSDT 2025å¹´ä¹‹åçš„æ•°æ®æ˜¯å¾®ç§’çº§ï¼Œä¹‹å‰æ˜¯æ¯«ç§’çº§)
def normalize_timestamp_safe(ts):
    """å®‰å…¨çš„æ—¶é—´æˆ³æ ‡å‡†åŒ–å‡½æ•°"""
    # å¤„ç† NaN
    if pd.isna(ts):
        return pd.NA
    # å¤„ç†å­—ç¬¦ä¸²
    if isinstance(ts, str):
        try:
            ts = float(ts.strip())
        except (ValueError, TypeError):
            return pd.NA
    # ç¡®ä¿æ˜¯æ•°å€¼
    try:
        ts = float(ts)
    except (ValueError, TypeError):
        return pd.NA
    # æ ‡å‡†åŒ–æ—¶é—´æˆ³
    if ts > 1e15:  # å¾®ç§’ â†’ æ¯«ç§’
        return ts // 1000
    elif ts > 1e12:  # æ¯«ç§’ â†’ ä¿ç•™
        return ts
    elif ts > 1e9:  # ç§’ â†’ æ¯«ç§’
        return ts * 1000
    else:
        return pd.NA  # æ— æ•ˆçš„å°æ•°å€¼

# ä¿®å¤CSVæ–‡ä»¶
def data_format_repair(symbol, interval, config, logger):
    dirs = get_data_dirs(symbol, interval, config)
    processed_dir = dirs["processed"]
    input_file = processed_dir / f"{symbol}_{interval}_raw.csv"

    if not input_file.exists():
        logger.error(f"æ–‡ä»¶ä¸å­˜åœ¨: {input_file}")
        return None
    df = pd.read_csv(input_file)
    # print(df)
    # å…³é”®ï¼šå…ˆè½¬æ¢ä¸ºæ•°å€¼ç±»å‹ï¼Œæ— æ•ˆå€¼è®¾ä¸º NaN
    df['open_time'] = pd.to_numeric(df['open_time'], errors='coerce')
    df['close_time'] = pd.to_numeric(df['close_time'], errors='coerce')
    # print(df)
    # ä¿®å¤æ—¶é—´æˆ³
    df['open_time'] = df['open_time'].apply(normalize_timestamp_safe)
    df['close_time'] = df['close_time'].apply(normalize_timestamp_safe)
    # print(df)

    fixed_file = processed_dir / f"{symbol}_{interval}_clean.csv"
    df.to_csv(fixed_file, index=False, header=True)
    logger.info(f"ä¿®å¤å®Œæˆ: {fixed_file}")
    return str(fixed_file)

# è®¾å®šæ˜ç¡®çš„æ—¶é—´è¾¹ç•Œï¼ˆæ¨èç”¨äºé‡‘èæ•°æ®ï¼ï¼‰ä¾‹å¦‚:
# train_end_date = "2024-01-01"
# val_end_date = "2024-12-31"
def split_data_by_datetime(df, train_end_date, val_end_date):
    # åˆ†å‰²df
    train_df = df[df['open_time'] <= train_end_date]
    val_df = df[(df['open_time'] > train_end_date) & (df['open_time'] <= val_end_date)]
    test_df = df[df['open_time'] > val_end_date]
    print("è®­ç»ƒé›†æ—¶é—´æ®µ:", train_df['open_time'].min(), "â†’", train_df['open_time'].max())
    print("éªŒè¯é›†æ—¶é—´æ®µ:", val_df['open_time'].min(), "â†’", val_df['open_time'].max())
    print("æµ‹è¯•é›†æ—¶é—´æ®µ:", test_df['open_time'].min(), "â†’", test_df['open_time'].max())
    return train_df, val_df, test_df


# ç”»å‡ºæ•°æ®é›†åˆ‡åˆ†å›¾åƒ
def plot_data_split(crypt_name, train_df, val_df, test_df, datasets_dir):
    plt.figure(figsize=(12, 4))
    # print(type(train_df['open_time']))
    # print(type(train_df['close']))
    plt.plot(train_df['open_time'], train_df['close'], label='Train', color='blue')
    plt.plot(val_df['open_time'], val_df['close'], label='Validation', color='orange')
    plt.plot(test_df['open_time'], test_df['close'], label='Test', color='red')
    # print(train_df['close'])
    plt.axvline(val_df['open_time'].min(), color='k', linestyle='--', alpha=0.5)
    plt.axvline(test_df['open_time'].min(), color='k', linestyle='--', alpha=0.5)
    plt.legend()
    plt.title(f'{crypt_name} SPLIT')
    plt.xlabel("Date")
    plt.ylabel("Close Price")
    #plt.show()
    plot_path = Path(datasets_dir) / f"{crypt_name}_split.png"
    plt.savefig(plot_path, dpi=150, bbox_inches='tight')
    plt.close()
    return plot_path


# æ•°æ®é›†åˆ‡åˆ†ä¸»å‡½æ•°
def data_split(symbol, interval, config, logger):
    processed_dir = PROJECT_ROOT / config["paths"]["processed_dir"]
    input_file = processed_dir / f"{symbol}_{interval}_clean.csv"

    if not input_file.exists():
        logger.error(f"æ–‡ä»¶ä¸å­˜åœ¨: {input_file}ï¼Œè¯·å…ˆè¿è¡Œä¿®å¤æ­¥éª¤")
        return

    df = pd.read_csv(input_file)
    df["open_time"] = pd.to_datetime(df["open_time"], unit="ms")
    df["close_time"] = pd.to_datetime(df["close_time"], unit="ms")

    train_df, val_df, test_df = split_data_by_datetime(
        df,
        config["split"]["train_end"],
        config["split"]["val_end"]
    )

    dataset_dir = PROJECT_ROOT / config["paths"]["datasets_dir"] / f"{symbol}_{interval}"
    dataset_dir.mkdir(parents=True, exist_ok=True)

    train_df.to_csv(dataset_dir / "train.csv", index=False)
    val_df.to_csv(dataset_dir / "val.csv", index=False)
    test_df.to_csv(dataset_dir / "test.csv", index=False)

    logger.info(f"æ•°æ®é›†åˆ’åˆ†å®Œæˆ | Train: {len(train_df)} Val: {len(val_df)} Test: {len(test_df)}")

    plot_path = plot_data_split(f"{symbol}_{interval}", train_df, val_df, test_df, dataset_dir)
    logger.info(f"åˆ’åˆ†å›¾å·²ä¿å­˜: {plot_path}")