# ä¸‹è½½ã€ä¿®å¤ã€æ‹†åˆ†æ•°æ®æ‰€éœ€è¦ç”¨åˆ°çš„å‡½æ•°
import sys
from pathlib import Path
# å°†é¡¹ç›®æ ¹ç›®å½•åŠ å…¥ sys.pathï¼ˆä»¥ä¾¿å¯¼å…¥ tools/ï¼‰
PROJECT_ROOT = Path(__file__).parent.parent
sys.path.insert(0, str(PROJECT_ROOT))
import os
import requests
from datetime import datetime
from dateutil.relativedelta import relativedelta
import pandas as pd
import glob
import zipfile
from pathlib import Path
import matplotlib.pyplot as plt
import yaml
# å¯¼å…¥æ—¥å¿—å·¥å…·
from tools.log_utils import setup_logger

def load_config(config_path="config.yaml"):
    """åŠ è½½é…ç½®æ–‡ä»¶"""
    config_path = Path(config_path)
    if not config_path.is_absolute():
        config_path = PROJECT_ROOT / config_path
    with open(config_path, "r", encoding="utf-8") as f:
        return yaml.safe_load(f)

def get_data_dirs(symbol, interval, config):
    """æ ¹æ®é…ç½®è·å–æ•°æ®ç›®å½•"""
    paths = config["paths"]
    return {
        "raw": PROJECT_ROOT / paths["raw_dir"] / symbol / interval,
        "processed": PROJECT_ROOT / paths["processed_dir"],
        "datasets": PROJECT_ROOT / paths["datasets_dir"] / f"{symbol}_{interval}"
    }


# =============== æ ¸å¿ƒå‡½æ•° ===============
# ä»Binanceä¸‹è½½åŸå§‹æ•°æ®ï¼Œæœ€å¤š3æ¬¡å°è¯•é‡æ–°ä¸‹è½½
def download_binance_klines(symbol, interval, config, logger, timeframe="monthly", max_retries=3):
    dirs = get_data_dirs(symbol, interval, config)
    # åˆ›å»ºä¿å­˜ç›®å½•
    save_dir = dirs["raw"]
    save_dir.mkdir(parents=True, exist_ok=True)
    # è§£æèµ·æ­¢æ—¶é—´
    start = datetime.strptime(config["data"]["start_date"], "%Y-%m")
    end = datetime.strptime(config["data"]["end_date"], "%Y-%m")
    current = start
    logger.info(f"å¼€å§‹ä¸‹è½½ {symbol} {interval} æ•°æ®")
    failed_files = []  # è®°å½•å¤±è´¥çš„æ–‡ä»¶
    while current <= end:
        year, month = current.year, current.month
        filename = f"{symbol}-{interval}-{year}-{month:02d}.zip"
        url = f"https://data.binance.vision/data/spot/{timeframe}/klines/{symbol}/{interval}/{filename}"
        filepath = save_dir / filename

        # æ£€æŸ¥æ–‡ä»¶æ˜¯å¦å·²å­˜åœ¨ä¸”å®Œæ•´
        if filepath.exists():
            if is_valid_zip(filepath):
                logger.info(f"âœ… å·²å­˜åœ¨ä¸”æœ‰æ•ˆ: {filename}")
                current += relativedelta(months=1)
                continue
            else:
                logger.warning(f"âš ï¸ æ–‡ä»¶æŸåï¼Œé‡æ–°ä¸‹è½½: {filename}")
                filepath.unlink()  # åˆ é™¤æŸåæ–‡ä»¶

        # å°è¯•ä¸‹è½½ï¼ˆå¸¦é‡è¯•ï¼‰
        success = False
        for attempt in range(max_retries):
            try:
                logger.info(f"ğŸ“¥ ä¸‹è½½ {filename} (å°è¯• {attempt + 1}/{max_retries})")
                response = requests.get(url, stream=True, timeout=30)
                if response.status_code == 200:
                    with open(filepath, "wb") as f:
                        for chunk in response.iter_content(chunk_size=8192):
                            f.write(chunk)

                    # éªŒè¯ä¸‹è½½çš„æ–‡ä»¶
                    if is_valid_zip(filepath):
                        logger.info(f"âœ… ä¸‹è½½æˆåŠŸ: {filename}")
                        success = True
                        break
                    else:
                        logger.warning(f"âš ï¸ æ–‡ä»¶æ— æ•ˆï¼Œé‡è¯•: {filename}")
                        if filepath.exists():
                            filepath.unlink()
                else:
                    logger.warning(f"âŒ HTTP {response.status_code}: {filename}")
            except Exception as e:
                logger.error(f"âš ï¸ ä¸‹è½½å¼‚å¸¸ {filename} (å°è¯• {attempt + 1}): {e}")
                if filepath.exists():
                    filepath.unlink()  # åˆ é™¤å¯èƒ½æŸåçš„æ–‡ä»¶

        if not success:
            logger.error(f"âŒ æœ€ç»ˆä¸‹è½½å¤±è´¥: {filename}")
            failed_files.append(filename)

        current += relativedelta(months=1)
    # æŠ¥å‘Šå¤±è´¥æƒ…å†µ
    if failed_files:
        logger.warning(f"âš ï¸ {len(failed_files)} ä¸ªæ–‡ä»¶ä¸‹è½½å¤±è´¥: {failed_files}")
    else:
        logger.info("âœ… æ‰€æœ‰æ–‡ä»¶ä¸‹è½½å®Œæˆ")
    return len(failed_files) == 0  # è¿”å›æ˜¯å¦å…¨éƒ¨æˆåŠŸ

def is_valid_zip(filepath):
    """éªŒè¯ ZIP æ–‡ä»¶æ˜¯å¦å®Œæ•´"""
    try:
        with zipfile.ZipFile(filepath, 'r') as z:
            # æ£€æŸ¥ ZIP æ–‡ä»¶å®Œæ•´æ€§
            bad_file = z.testzip()
            return bad_file is None
    except (zipfile.BadZipFile, OSError):
        return False

# å¤šä¸ª.zipæ–‡ä»¶åˆå¹¶ä¸ºCSVæ–‡ä»¶
def merge_binance_klines(symbol, interval, config, logger):
    dirs = get_data_dirs(symbol, interval, config)
    raw_dir = dirs["raw"]
    processed_dir = dirs["processed"]
    processed_dir.mkdir(parents=True, exist_ok=True)

    zip_files = sorted(glob.glob(str(raw_dir / "*.zip")))
    if not zip_files:
        logger.error(f"æœªæ‰¾åˆ° ZIP æ–‡ä»¶: {raw_dir}")
        return None

    dfs = []
    for zip_path in zip_files:
        with zipfile.ZipFile(zip_path, 'r') as z:
            for csv_name in z.namelist():
                if csv_name.endswith('.csv'):
                    with z.open(csv_name) as f:
                        df = pd.read_csv(f, header=None)
                        dfs.append(df)
                        logger.debug(f"è¯»å–: {csv_name}")

    if not dfs:
        logger.error("æœªæ‰¾åˆ°ä»»ä½• CSV æ–‡ä»¶")
        return None

    full_df = pd.concat(dfs, ignore_index=True)
    full_df.columns = [
        'open_time', 'open', 'high', 'low', 'close', 'volume',
        'close_time', 'quote_asset_volume', 'number_of_trades',
        'taker_buy_base_volume', 'taker_buy_quote_volume', 'ignore'
    ]
    output_file = processed_dir / f"{symbol}_{interval}_raw.csv"
    full_df.to_csv(output_file, index=False)
    logger.info(f"åˆå¹¶å®Œæˆ: {output_file} ({len(full_df)} è¡Œ)")
    return str(output_file)

# ä¿®å¤æ—¶é—´æˆ³å‡½æ•° (âœ…BTCUSDT 2025å¹´ä¹‹åçš„æ•°æ®æ˜¯å¾®ç§’çº§ï¼Œä¹‹å‰æ˜¯æ¯«ç§’çº§)
def normalize_timestamp(ts):
    """å®‰å…¨çš„æ—¶é—´æˆ³æ ‡å‡†åŒ–å‡½æ•°"""
    # å¦‚æœæ˜¯å­—ç¬¦ä¸²ï¼Œå°è¯•è½¬æ¢ä¸ºæ•°å€¼
    if isinstance(ts, str):
        try:
            ts = float(ts)
        except (ValueError, TypeError):
            return pd.NA  # æ— æ³•è½¬æ¢çš„å­—ç¬¦ä¸²è¿”å› NaN
    # å¦‚æœæ˜¯ NaN æˆ– None
    if pd.isna(ts):
        return ts
    # ç°åœ¨ç¡®ä¿ ts æ˜¯æ•°å€¼ç±»å‹
    try:
        ts = float(ts)
    except (ValueError, TypeError):
        return pd.NA
    # æ ‡å‡†åŒ–æ—¶é—´æˆ³
    if ts > 1e15:  # å¾®ç§’ â†’ æ¯«ç§’
        return ts // 1000
    elif ts > 1e12:  # æ¯«ç§’ â†’ ä¿ç•™
        return ts
    elif ts > 1e9:  # ç§’ â†’ æ¯«ç§’
        return ts * 1000
    else:
        return ts

# ä¿®å¤CSVæ–‡ä»¶
def data_format_repair(symbol, interval, config, logger):
    dirs = get_data_dirs(symbol, interval, config)
    processed_dir = dirs["processed"]
    input_file = processed_dir / f"{symbol}_{interval}_raw.csv"

    if not input_file.exists():
        logger.error(f"æ–‡ä»¶ä¸å­˜åœ¨: {input_file}")
        return None

    df = pd.read_csv(input_file, header=None)
    df.columns = [
        'open_time', 'open', 'high', 'low', 'close', 'volume',
        'close_time', 'quote_asset_volume', 'number_of_trades',
        'taker_buy_base_volume', 'taker_buy_quote_volume', 'ignore'
    ]

    df['open_time'] = df['open_time'].apply(normalize_timestamp)
    df['close_time'] = df['close_time'].apply(normalize_timestamp)

    fixed_file = processed_dir / f"{symbol}_{interval}_clean.csv"
    df.to_csv(fixed_file, index=False)
    logger.info(f"ä¿®å¤å®Œæˆ: {fixed_file}")
    return str(fixed_file)

# è®¾å®šæ˜ç¡®çš„æ—¶é—´è¾¹ç•Œï¼ˆæ¨èç”¨äºé‡‘èæ•°æ®ï¼ï¼‰ä¾‹å¦‚:
# train_end_date = "2024-01-01"
# val_end_date = "2024-12-31"
def split_data_by_datetime(df, train_end_date, val_end_date):
    # åˆ†å‰²df
    train_df = df[df['open_time'] <= train_end_date]
    val_df = df[(df['open_time'] > train_end_date) & (df['open_time'] <= val_end_date)]
    test_df = df[df['open_time'] > val_end_date]
    print("è®­ç»ƒé›†æ—¶é—´æ®µ:", train_df['open_time'].min(), "â†’", train_df['open_time'].max())
    print("éªŒè¯é›†æ—¶é—´æ®µ:", val_df['open_time'].min(), "â†’", val_df['open_time'].max())
    print("æµ‹è¯•é›†æ—¶é—´æ®µ:", test_df['open_time'].min(), "â†’", test_df['open_time'].max())
    return train_df, val_df, test_df


# ç”»å‡ºæ•°æ®é›†åˆ‡åˆ†å›¾åƒ
def plot_data_split(crypt_name, train_df, val_df, test_df, datasets_dir):
    plt.figure(figsize=(12, 4))
    plt.plot(train_df['open_time'], train_df['close'], label='Train', color='blue')
    plt.plot(val_df['open_time'], val_df['close'], label='Validation', color='orange')
    plt.plot(test_df['open_time'], test_df['close'], label='Test', color='red')
    print(train_df['close'])
    plt.axvline(val_df['open_time'].min(), color='k', linestyle='--', alpha=0.5)
    plt.axvline(test_df['open_time'].min(), color='k', linestyle='--', alpha=0.5)
    plt.legend()
    plt.title(f'{crypt_name} SPLIT')
    plt.xlabel("Date")
    plt.ylabel("Close Price")
    #plt.show()
    plot_path = Path(datasets_dir) / f"{crypt_name}_split.png"
    plt.savefig(plot_path, dpi=150, bbox_inches='tight')
    plt.close()
    return plot_path


# æ•°æ®é›†åˆ‡åˆ†ä¸»å‡½æ•°
def data_split(symbol, interval, config, logger):
    processed_dir = PROJECT_ROOT / config["paths"]["processed_dir"]
    input_file = processed_dir / f"{symbol}_{interval}_clean.csv"

    if not input_file.exists():
        logger.error(f"æ–‡ä»¶ä¸å­˜åœ¨: {input_file}ï¼Œè¯·å…ˆè¿è¡Œä¿®å¤æ­¥éª¤")
        return

    df = pd.read_csv(input_file)
    df["open_time"] = pd.to_datetime(df["open_time"], unit="ms")
    df["close_time"] = pd.to_datetime(df["close_time"], unit="ms")

    train_df, val_df, test_df = split_data_by_datetime(
        df,
        config["split"]["train_end"],
        config["split"]["val_end"]
    )

    dataset_dir = PROJECT_ROOT / config["paths"]["datasets_dir"] / f"{symbol}_{interval}"
    dataset_dir.mkdir(parents=True, exist_ok=True)

    train_df.to_csv(dataset_dir / "train.csv", index=False)
    val_df.to_csv(dataset_dir / "val.csv", index=False)
    test_df.to_csv(dataset_dir / "test.csv", index=False)

    logger.info(f"æ•°æ®é›†åˆ’åˆ†å®Œæˆ | Train: {len(train_df)} Val: {len(val_df)} Test: {len(test_df)}")

    plot_path = plot_data_split(f"{symbol}_{interval}", train_df, val_df, test_df, dataset_dir)
    logger.info(f"åˆ’åˆ†å›¾å·²ä¿å­˜: {plot_path}")